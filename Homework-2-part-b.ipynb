{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Classification\n",
    "\n",
    "### <font color='red'>Submit before the deadline as no late submission is accepted.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables:\n",
    "\n",
    "- Submit your answers to conceptual questions (described in another file) in a pdf file\n",
    "- Write down your codes in the given cells described in this file, denoted as \"YOUR CODE HERE\"\n",
    "- Write down your discussion in the given cells, denoted as \"YOUR DISCUSSION HERE\"\n",
    "- Submit two files: .pdf and .ipynb files to eLearning\n",
    "\n",
    "\n",
    "This assignment covers Supervised Learning models. In this assignment, you are required to use one clean dataset to train FOUR classification models for discrete targets.\n",
    "\n",
    "\n",
    "The total score of the implementation part is: 70 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tarun Chegondi Naga Sri Narahari Sai'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME = \"Tarun Chegondi Naga Sri Narahari Sai\"\n",
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "For this section of the assignment we will be working with the [UCI Mushroom Data Set](http://archive.ics.uci.edu/ml/datasets/Mushroom?ref=datanews.io) stored in `mushrooms.csv`. The data will be used to train a model to predict whether or not a mushroom is poisonous. The following attributes are provided:\n",
    "\n",
    "*Attribute Information:*\n",
    "\n",
    "1. cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s \n",
    "2. cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s \n",
    "3. cap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y \n",
    "4. bruises?: bruises=t, no=f \n",
    "5. odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s \n",
    "6. gill-attachment: attached=a, descending=d, free=f, notched=n \n",
    "7. gill-spacing: close=c, crowded=w, distant=d \n",
    "8. gill-size: broad=b, narrow=n \n",
    "9. gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y \n",
    "10. stalk-shape: enlarging=e, tapering=t \n",
    "11. stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=? \n",
    "12. stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s \n",
    "13. stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s \n",
    "14. stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y \n",
    "15. stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y \n",
    "16. veil-type: partial=p, universal=u \n",
    "17. veil-color: brown=n, orange=o, white=w, yellow=y \n",
    "18. ring-number: none=n, one=o, two=t \n",
    "19. ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z \n",
    "20. spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y \n",
    "21. population: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y \n",
    "22. habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d\n",
    "\n",
    "<br>\n",
    "\n",
    "The data in the mushrooms dataset is currently encoded with strings. We need to convert the categorical variables into numeric indicators. One way to achieve this is by using the pd.get_dummies function, which will create indicator variables for each category in the data, effectively converting them to a numeric representation that can be used by the algorithms in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mush = pd.read_csv('mushrooms.csv')\n",
    "cat_mush = pd.get_dummies(mush)\n",
    "\n",
    "X_mush = cat_mush.iloc[:,2:]\n",
    "y_mush = cat_mush.iloc[:,1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mush, y_mush, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6093"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the figure, the two classes does not seem to be linearly separable and can create some challenges for classification. Let us try different models to complete the classification task and check their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (10 points)\n",
    "- Train a DecisionTreeClassifier with default parameters and random_state=0. \n",
    "- What are the 5 most important features found by the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odor_n: 0.6251435175471661\n",
      "stalk-root_c: 0.1691757144252228\n",
      "stalk-root_r: 0.08658915843078754\n",
      "spore-print-color_r: 0.03437506344670402\n",
      "odor_l: 0.023503682936672883\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE (10 points)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=0)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve and sort feature importances\n",
    "importances = dt_classifier.feature_importances_\n",
    "feature_names = X_mush.columns\n",
    "important_features = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Print the most important features\n",
    "for feature, importance in important_features:\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (10 points)\n",
    "- Train a Decision Tree model. Set \"max_depth\" to 6, \"min_samples_split\" to 2, \"max_leaf_nodes\" to  10, and random_state to 0.\n",
    "- Report the test accuracy of the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE (10 points)\n",
    "# Train the Decision Tree model with specified parameters\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=6, min_samples_split=2, max_leaf_nodes=10, random_state=0)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (15 points)\n",
    "- Train a linear svm classifier with grid search and cross-validation. Set random_state as 0. Let the choices of C be: [0.001, 0.01, 0.1, 1, 10, 100, 10000, 1000000]. Use 5-fold cross-validation.\n",
    "- Report (1) the best C chosen, (2) the test accuracy under the best model, and (3) the mean validation accuracy through the cross-validation process (under the best model).\n",
    "- Given the choice, discuss briefly: do you think a hard-margin SVM can outperform soft-margin SVM in this case? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1\n",
      "Mean Validation Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE (10 points)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 10000, 1000000]}\n",
    "\n",
    "# Set up the GridSearch with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(SVC(random_state=0), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "best_C = grid_search.best_params_['C']\n",
    "best_score = grid_search.best_score_\n",
    "test_accuracy = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best C: {best_C}\")\n",
    "print(f\"Mean Validation Accuracy: {best_score}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR DISCUSSION HERE (5 points)\n",
    "In the case of linearly separable data, a hard-margin SVM might perform better than a soft-margin SVM because it tries to find the maximum-margin hyperplane without allowing any margin violations. However, in real-world datasets like the mushroom dataset, where the classes may not be perfectly separable, a soft-margin SVM is often preferred.\n",
    "A soft-margin SVM allows for some margin violations, which can lead to better generalization to unseen data and can handle noisy or overlapping data points more effectively. It introduces a penalty term (C) that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "In this case, since the mushroom dataset contains categorical features with many classes and may not be perfectly separable, a soft-margin SVM is likely to be more appropriate. The grid search results and the test accuracy obtained under the best model can provide insights into the effectiveness of the chosen C value and the performance of the SVM classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (10 points)\n",
    "- Train\n",
    "a kernel svm classifier with grid search and cross-validation. Set random_state as 0. Let's apply rbf kernel. Let the choices of C be: [0.1, 1, 10]. Let the choices of gamma be: [0.0001, 0.001, 0.01, 0.1, 1, 10]. Use 5-fold cross-validation.\n",
    "- Report (1) the best C and gamma chosen, (2) the test accuracy under the best model, and (3) the mean validation accuracy through the cross-validation process (under the best model).\n",
    "- Note: The code may take up to several minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'gamma': 0.1}\n",
      "Mean Validation Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Adjust the parameter grid for C and gamma\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Set up the GridSearch with the rbf kernel\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf', random_state=0), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "test_accuracy = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Mean Validation Accuracy: {best_score}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. Ensemble Methods - Random Forest (15 points)\n",
    "- Train a random forest model. Specifically, train 100 decision trees (i.e., n_estimators=100). For each tree, set max_depth = 6, min_samples_split = 2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=6, min_samples_split=2, random_state=0)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate the accuracy\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (10 Points)\n",
    "- Compare the mean validation score across all models. If we would like to choose one model for prediction based on model performance (i.e., in this case, accuracy), which one would you choose? Explain briefly."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR DISCUSSION HERE\n",
    "With all models achieving a test accuracy of 1.0, the decision of which model to choose for deployment depends on other factors beyond accuracy. Hereâ€™s a summary of the key aspects of each model and a recommendation:\n",
    "\n",
    "### Model Performance Overview\n",
    "\n",
    "1. Decision Tree (Custom Settings)\n",
    "   Configuration**: `max_depth=6`, `min_samples_split=2`, `max_leaf_nodes=10`, `random_state=0`\n",
    "   Test Accuracy**: 1.0\n",
    "   Pros: High interpretability; easy to understand decision paths.\n",
    "   Cons: Can be sensitive to small changes in data (although less so with these settings).\n",
    "\n",
    "2. Linear SVM\n",
    "   Configuration: Grid search with `C` values `[0.001, 0.01, 0.1, 1, 10, 100, 10000, 1000000]`, `random_state=0`\n",
    "   Best C: 1\n",
    "   Mean Validation Accuracy: 1.0\n",
    "   Test Accuracy: 1.0\n",
    "   Pros: Effective in high-dimensional spaces; good generalization capabilities.\n",
    "   Cons: Less interpretable than decision trees; computational cost can be high with large datasets            and complex models.\n",
    "\n",
    "3. Random Forest\n",
    "   Configuration: 100 trees, `max_depth=6`, `min_samples_split=2`\n",
    "   Test Accuracy: 1.0\n",
    "   Pros: Robust against overfitting compared to a single decision tree; handles both bias and                  variance efficiently.\n",
    "   Cons: Lower interpretability than a single decision tree; higher computational cost than                    individual trees.\n",
    " \n",
    "Finally, the Random Forest model would generally be the best choice in scenarios where model robustness and generalization are priorities, and where computational resources allow for somewhat more complex models. Its ability to maintain high accuracy across different data samples and its relative robustness to overfitting make it suitable for reliable, ongoing use in production environments, especially when the interpretability of a single decision tree is not a stringent requirement.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
